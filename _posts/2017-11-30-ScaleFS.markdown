---
layout: default
title:  "ScaleFS分析"
date:   2017-10-29
categories: filesystem
---

ScaleFS是SOSP17，MIT做的一个多核文件系统，简单分析下他的设计方案。

## 总体架构

![]({{ site.baseurl }}/assets/scalefs-1.png)

ScaleFS包括2个FS，一个MEMFS，一个DiskFS。

- MEMFS：使用并发内存数据结构，实现内存操作的多核扩展，让多数操作免锁
- DISKFS：组织磁盘块，实现空间管理

2个文件系统之间通过一个op-log关联；这个log保存目录的所有变化，包括link，unlink，rename等。当需要刷盘时，直接从oplog收集这些操作，并下盘。

MEMFS与DiskFS相互独立，使用不同的inode号，为了方便区分，文中未mnode代表MEMFS的inode，inode代表DISK中的inode。有个表格记录mnode到inode的映射。

这样使得mnode的分配速度非常快，不需要访问磁盘，比如可以每个核分配一个mnode范围，这样也无需加锁。但是在重启之后，stat就会发现inode有变化。目前来看，依赖inode号不变的应用不多（主要指本地FS，NFS还是必须要inode号的）。

MEMFS包括：

- 一个page cache，每个file使用一个radix array来存放热点数据，脏的写也在这里，通过bitmap标识。
- 一个dentry hash表，映射文件名到mnode号；这个表是无锁的，对于每个dentry条目的修改是需要加锁的。另外有一篇论文专门讲这个。

DiskFS：

- 实现一个传统的磁盘文件系统，使用物理journal来将修改刷到磁盘中；为了更好的扩展性，DiskFS使用多个物理journal。
- DiskFS自己也维护一个buffer，缓存一些磁盘数据结构，比如空间分配表，inode表之类的；但是DiskFS并不做数据的cache。

一个oplog层来实现2层之间的交互，他主要做以下事情：

## 细节设计

### 记录操作的顺序

为了保证crash一致性，所有的操作必须按照一定的顺序加入到MEMFS的log中去。

所有的读操作都是不加锁的，使得如何确定不同操作的顺序会比较复杂。

比如一个场景，我们首先将每个操作log起来，log的过程中将需要的信息加锁，log完成之后解锁。

考虑一个简单的场景：

- dir1包含3个file names，a，b，c. a对应inode 1，b和c都对应inode 2.
- 线程T1提交rename调用，rename(b,c);线程T2提交rename(a,c).
- 假设线程T1先来，那么T1执行无锁的读取b和c，发现2者是同一个inode，那么直接删除dentry b，并减inode的引用；整个过程仅仅需要对b加锁。
- 接着T2过来，获取a和c的锁去执行rename；故T1和T2之间没有锁冲突，此时2者并发执行
- 由于T1先执行，T2后执行，此时MEMFS中c指向的inode为a的，即为1；
- 由于并发执行，可能T2先log完成，那么重放到盘之后，inode为b的，即为2，造成不一致。

ScaleFS的解决方案如下：

- 现代的CPU能够在不同的核之间提供准确的时间戳，这样就可以实现全局的oplog了
- 时间戳计数能够通过RDTSC命令获取
- 再这样的假设下，SCALEFS将内存中的dentry操作全局排序；即老的时间戳还没刷下去之前，新的时间戳的不让下去


基于时间戳的无所读：对于上面那个例子，需要保证T1 rename（b，c）的序列化点一定在T2 rename（a，c）之前，即T1读取到的 c一定在T2修改c之前。

- 为了确保在写的过程中，实现无锁的读取，MEMFS加了一个seqlock的逻辑（类似Linux内核的实现呢）；确保读发生在写操作之前。
- 使用seqlock的时候，写操作会对share的数据增加seqnum，写完再增加一次；读的开始和结束都会检查这个num，如果有变化，说明读的过程中有修改
- 如果发现过程中有修改，读retry，直到这个号不变
- MEMFS在修改之前都需要先读取，且能保证读取的都是正确的数据
- 每个读操作都有reqlock保护，并在其中读取时间戳；时间戳是一个很好的同步点（LP）

### merge操作

LP点能够很容易的实现多核之间的线性顺序，但是merge操作仍然要十分小心。

![]({{ site.baseurl }}/assets/scalefs-1.png)

- SCALEFS的每个核都有一个log，在fsync或sync的时候把多个核上的修改merge到一起。
- 假设2个并发任务op1和op2，分别在core1和core2执行；op1的LP先于op2，但是op2的log先完成，就上面描述的例子一样
- 如果这个时候刷log，就会造成不一致，如何解决：
  - 在merge的时候，检查所有人正在进行中的任务，如果有任务的LP点，比当前merge的操作要早，那么需要等待这个任务log之后才能刷下去

### 并发fsync动作

在fsync之前，每个核自行处理各自的逻辑，当调用fsync时，需要将多个核上的log merge到一起；为了避免每个核一个大log导致fsync的性能平均，MEMFS为每个mnode提供了一个独立的log。这样就很容易将多个不同核上关于某个特定mnode的修改快速merge。

### flash一个oplog

scalefs很容易找到针对一个mnode上的所有oplog，并将这些刷到盘上。但是刷盘的时候要考虑几个问题：

- 对于多个相关联的操作，要能够合并到一起下盘，提升性能
- 需要处理rename这种动作，避免出现文件丢失
- 需要确保盘上的文件系统是一致的，不包括孤儿dentry，孤儿目录，link到无效的地方等

flush流程如下：

- 首先计算需要持久化的所有内容，并记录到物理日志中；如果多个操作集合大于物理日志大小，那么需要通过多个batch下盘；物理日志大小至少能满足一个操作的需要。
- 将所有的操作按照时间戳排序，从最旧到最新的顺序下盘

修改吸收：

- 对于同一个内容的多次修改，直接在内存合并，提升性能
- 比如刚创建的文件就删除掉，那么fsync的时候什么也不用做；很多临时的应用有这样的场景
- ScaleFS不处理mtime和atime，用于提升性能
- 需要注意的是：有些应用打开了一个文件，然后这个文件被删除，但是fd依然保留的情况；对于这种动作，fsync也是啥也不干

跨目录的rename：

- 对了防止跨目录rename导致文件丢失，ScaleFS会记录操作之间的关联关系
- 在fsync一个正在rename的目录d时，不管是rename的源目录还是目标目录，还需要flush掉另外一个目录的操作；这就需要访问另外一个目录的日志信息了
- 并不用flush这个目录的所有操作，仅仅flush到rename那个时间；当然也可以flush全量，可以方便多做合并
- 通过时间戳的方式避免循环的依赖：依赖关系仅仅针对一个特定的mnode，而且仅仅是这个特定的mnode的一个特定的时间戳

内部的一致性：scalefs必须确保内部在crash之后的一致性，有三种情况必须考虑：

- 必须确保目录links数量就是盘上这个目录中的inode的数量，每个link都确实在盘上存在
  - 需要在每个文件flush之后才建立link，通常情况下，flush文件和创建link在一个事务中完成
  - 但是当一个目录巨大的时候，无法在一个事务完成时：此时需要启动多个事务，在这种情况下，必须先创建flush文件，再创建link
- 必须确保没有孤儿目录项
  - 当递归删除目录时，调用父目录的fsync，可能导致孤儿存在，此时必须等所有的删除都flush完成之后，才能删除父目录
- 必须确保盘上文件系统在crash恢复之后不存在loop（出现目录循坏），这个需要重点阐述下。

![]({{ site.baseurl }}/assets/scalefs-1.png)

如图的流程：

- 2个mv动作之后，内存的如最右侧所示
- 如果此时调用fsync D，那么fsync会把A目录的修改也刷下来，此时会造成盘上的loop出现；因为针对B的修改没有刷下来
- 为了避免这种问题，需要制定如下规则：
  - 在内存中夸目录的rename需要加一个大锁
  - 当一个目录被移动在目录d中，scalefs记录从d到root的整个path（在锁的范围内）
  - 当flush目录d时，如果有子目录move到d，scalefs先将之前记录的整个path的修改全部刷下去；这样能否确保d的孩子不会成为d的父亲

