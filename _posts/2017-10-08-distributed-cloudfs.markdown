---
layout: default
title:  "DeltaFS分析"
date:   2017-09-24
categories: filesystem
---

## 设计原则和假设条件

- 每个租户一个独立的名字空间，占用独立的资源
- 默认单个租户文件系统是一个从小长大的过程（也不排除租户申请FS好之后，导入大量的数据）
- VM的计算网络比存储网络差很远（一般存储网络默认N * 10GE以上，计算侧VM之间仅为1GE），即尽量不使用计算侧VM之间的通信
- 租户的负载具有一定程度的局部性，比如VM1更多使用子目录1，VM2更多使用子目录2，业务有一定程度的切割，比如gitlab后台不同的项目用不同的目录
- ls和rename之类的操作主要在单个节点内搞定，跨节点的ls和rename比较少，可以用相对低效的方式处理（第一个版本考虑不支持跨节点原子rename）

## 元数据管理

整体元数据布局与单机版本完全一致，通过rocksdb的表格来存放整个文件系统的所有元数据信息，key为父目录inode + name的hash；

![]({{ site.baseurl }}/assets/DFS-1.png)

### 元数据切割方式

- 用户申请CloudFS时，仅启动一个实例，后续整个目录树都在这个实例中
- 当用户存放的文件数量变多，FSM检测到负载瓶颈或者管理员手动操作（FSM维护最上面2层目录的负载特征情况），再触发事件申请一个新的CloudFS，优先迁移第一层目录到另外一个CloudFS
- 迁移完成后，更新FSM中的视图，删除旧的目录
- 也可以提供API，由管理员（用户）直接申请多个CloudFS实例，并提前将目录分配到不同的CloudFS实例，以方便用户一次性导入大量现有数据

### 子树迁移流程

- FSM发现负载瓶颈（或者管理员手动触发），并分析出需要迁移的目录，比如/dir2，原来的实例为CloudFS1
- FSM启动一个新的CloudFS2实例（在另外一个VM上启动），并对dir2加锁（防止迁移冲突），FSM将迁移请求发送到需要迁出的CloudFS实例
- 待迁出的CloudFS实例收到目录迁移请求，对dir2加写锁（迁移期间，禁止目录修改），将dir2的所有元数据扫出来直接写入到一个本地sstable文件中（按照sstable的格式），并通知FSM准备好了；
  - 或者将rocksDB的log刷到盘上，seal掉，直接把这些sstable同时link到CloudFS2？会有个问题，谁来主导回收，回收之后再同时link到2个CloudFS?
- FSM发送请求到新的CloudFS2实例（带上sstale的名字），CloudFS2实例直接将sstable link到他的table中
- FSM通知迁出的CloudFS1做清理工作：包括解锁dir2，删除dir2
- 整个迁移任务的原子性由FSM保证，中间出现错误由FSM来进行retry
- 迁移完成后，CloudFS1上存放除了dir2的子目录树，CloudFS2存放dir2的子树，视图由FSM维护
- CloudFS对外可以提供一个独立的地址，或者提供多个CloudFS实例的地址，FSM提供API供业务查询每个目录所在的CloudFS实例，方便上层业务实现局部性挂载（性能最优）

### 关键操作的流程

ls类操作：

- 如上面例子，迁移之后根目录下dir1在cloudfs1，dir2在cloudfs2，在根目录下ls
- 客户端通过查询FSM视图（一般都缓存在本地），知道根目录下的多个子目录在不同的cloudfs实例
- 发送ls到cloudfs1和cloudfs2，并将结果聚合之后返回客户端

rename操作：

- 本地的rename不在本文分析之列，本设计仅关注跨节点的rename，假设/dir1/foo要mv到/dir2中
- 简单的方案，让用户使用cp + rm代替rename，直接调用rename直接返回失败，这个对于特定的用户可行，但是不够通用
- 更通用的方案：
  - 如果是一个比较大的目录，可以考虑直接复用迁移的流程，将请求发给FSM，由FSM来主导整个过程
  - 如果目录很小（一个RPC就可以装下）：log这个操作到本地，锁定这个目录（写锁），读取这个目录所有信息，直接RPC将目录的所有信息发送给目标CloudFS2，cloudfs2创建整个目录树，CloudFS2返回成功，CloudFS删除旧的目录或文件，删除日志

## 数据路径

![]({{ site.baseurl }}/assets/DFS-1.png)

直接强依赖后台的共享存储，每个节点都能看到所有的数据信息，如果是EVS，则使用共享卷，将卷挂载到所有VM中.

数据布局如下：

- 小文件布局：小于64KB认为是小文件，申请一个巨大的地址空间，通过append的方式将写入的数据append到这个地址空间中，小文件在这个地址空间中的偏移存放在rocksdb表中
- 大文件布局：可以与小文件布局类似，按照64KB为粒度append到的地址空间中，此时大文件小IO写放大会非常严重，性能会比较挫；或者直接使用ext4类似的方式来管理空间；或者直接用ext4。不过当前我们面对的研发场景并不会有大文件小IO，HPC中可能会有这样的场景（可以考虑将大小文件分离存放，比如单独申请一个volume来存放大文件）。
- 小文件长成大文件：对于小文件长大到超过64KB，我们加锁，将小文件的几个分离的block复制到大文件中，指针指向大文件，解锁，标记删除小文件。
- 大文件缩成小文件：这类场景很少，暂时不做处理

## 几个关键流程设计

### 创建和挂载CloudFS

- 联系FSM，FSM发现第一次创建，将整个目录树给CloudFS1，即映射表为：(/, /cloudfs1/)启动CloudFS1实例，完成后返回实例地址
  - 启动过程中准备好rocksdb的table，用来存元数据
  - 创建一个大文件（默认100GB）用来append小文件数据（小于64KB）
  - 创建一个目录（后台使用本地fs的情况下）用来存放大文件
- NAS客户端使用实例地址挂载CloudFS
- 挂载成功后，创建/dir1流程如下：
  - CloudFS1查询FSM ‘/’ 路径的地址，查询之后发现就是CloudFS1自己，并缓存这个映射表
  - 在本地创建 /dir目录返回成功

### 创建 & 写入一个小文件

假设创建&写入文件为/dir1/file1

- 查询到/dir1归属CloudFS1，直接在本地创建/dir1/file1
- 写入数据到CloudFS1，CloudFS1直接将key + 数据append到一个大文件（或者大地址空间）中，并返回offset
- offset写入元数据table中，返回成功

### 删除小文件

- 直接标记数据的flag为删除，等待后台回收
- 删除元数据

### 修改小文件

- 与前面的数据有重叠的话，将老的数据复制与新的数据拼接，然后append到大文件的尾部
- 修改元数据table中的offset字段，指向新的地址
- 标记旧的条目为删除

整个流程不保证原子性，有可能存在数据已经修改，但是旧的数据没删除的情况，此时有几个方案：

- 回收空间的时候，检查下元数据是否真的指向这个地址，如果不是，标记为删除并回收
- 后台定时进程，全表扫描有效的条目是否真的有效
- 元数据表中填2个地址，一个新的一个旧的，修改的时候可以确保原子性

## 内存数据结构

- 客户端和server端在同一个进程中，客户端不做数据和元数据cache，仅做映射map的cache
- server端需要cache数据和元数据
- 元数据cache最为关键，将核心的一些操作需要的元数据cache起来，比如access，getattr，lookup之类的
- 内存数据结构包括：
  - 父inode + name的hash作为key，关键的元数据为value的hash表；使用多级hash + 换入换出的模式
  - inode作为key，value为父目录inode + 关键元数据（与上一个同一份内存）的Hash表，方便快速找到父目录信息

## 小结

整个正常流程的处理逻辑比较简单，异常的流程如何来做还需要分析，总体来看这个方案比较简单，也容易走通。

## 异常流程

所有的单机服务由K8S来保证高可用，事情的关键就是如何处理跨节点的失效问题。

## 跨节点迁移失效

假设在目录树迁移的过程中：
- FSM失效，FSM会被K8S自动拉起，并从log信息（etcd）中读取正在进行的迁移流程，并将整个流程重做一遍
- 如果在迁移过程中源CloudFS失效，此时FSM会感知到业务超时，并retry，源CloudFS恢复后，重新执行任务
- 如果在迁移过程中，目标CloudFS失效，FSM同样会感知到超时，并retry，目标CloudFS恢复之后，重新执行任务即可
- retry三次都失败之后，放弃迁移

假设rename的过程失效：
- 大目录流程与迁移完全一致
- 小目录mv，如果目标端失效，直接通过retry来搞定；如果源端失效，得源端重启后，读取log信息（意味着操作之前需要log这个renme动作），然后重新进行mv？（此时客户端是何种状态？）
  - 需要与协议对齐，如果源端故障，NAS客户端是否会自动retry这个rename操？如果协议处理这个问题，那么源端故障，就无需处理
