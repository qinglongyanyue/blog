---
layout: default
title:  "Haystack小文件优化存储分析"
date:   2018-08-23
categories: filesystem
---

# PolarFS分析

## 简介
PolarFS使用NVMe + SPDK技术，RDMA技术实现了一个文件系统，对外直接通过libpfs提供API访问，不是一个标准的Posix文件系统，专为阿里的PolarDB定制。

分布式存储资源池内部的副本一致性使用了阿里改进的并行RAFT算法实现，通过各种技术组合实现分布式存储到达类似本地存储的延迟（100us以内），极大的提升了数据库的性能体验。

核心逻辑是存储与数据库的分离架构，存储出类似Posix API给数据库来使用，这样数据库就无需做存储的数据可靠性之类的工作，同时类Posix API使得数据库基本不需要太大的修改就可以直接run上来。

同时由于存储资源池可以做的很大，可用性和可靠性会更高，这样很方便做多租户，每个租户实例无状态，方便DB实例的部署运维升级之类的动作。

![]({{ site.baseurl }}/assets/polarfs-1.png)

## 架构设计

![]({{ site.baseurl }}/assets/polarfs-2.png)

如果仅看架构，跟业界主流方案没有大的差异，看图的话，跟Google File System有一点点类似，集中的控制节点。

架构很明显的分为2层，上层是文件系统层，下层是基于RAFT的分布式资源池。

关键点：
- libpfs是一个用户态类Posix库，对外提供API
- PolarSwitch用来将IO转发到chunkserver
- Chunkserver存放数据
- PolarCtrl专业做各种master，后台直接用的mysql

### 文件系统层

![]({{ site.baseurl }}/assets/polarfs-3.png)

关键点：
- 文件系统对外暴露类Posix接口，方便上层业务使用
- 每个PolarDB实例对应一个存储Volume，volume ID全局唯一；并且PolarDB运行的Hostid也是个很重要的信息，后文详解。
- PolarFS文件系统就构建在这个存储Volume中（并不是块存储，仅仅是个逻辑的概念），这个volume全系统任何地方共享可见
- 挂载文件系统的时候将目录树，文件映射map和空间映射map加载到内存中，后文详解

### 存储层

本人对于文件系统实现更感兴趣，直接先分析文件系统后续再回来分析这个。

## 文件系统层的实现

文件系统层主要做2件事情：
1. 在一个数据库实例节点上组织元数据更并更数据和元数据信息
2. 在多个节点之间确保这些修改的一致性

![]({{ site.baseurl }}/assets/polarfs-4.png)

### 元数据组织
文件系统有三种类型的元数据：dentry，inode，空间映射。
- dentry组成文件系统目录树结构，每个dentry代表多层文件系统路径中的一层，每个dentry指向一个inode
- inode描述一个文件或者目录的基本元数据信息（各种时间，权限等），同时如果是文件记录文件对应的空间信息，如果是目录记录目录中的条目信息
- 空间映射信息是普通文件inode中的子项，当然还需要记录的空间状态

三种元数据都通过一种metaobject的东西来抽象，这个东西在盘上和在内存中都是一个概念. 在格式化文件系统的时候，metaobject直接就是盘上连续的4KB空间，每个4KB有个唯一的编号（最简单的，volumeid + offset）就可以作为这个唯一的编号。

挂载文件系统的时候，所有的metaobject都会加载到内存中并分成不同的组。

元数据在内存寻址方位为（metaobject id，metaobject type），后者用来找到对应的分组，前者用来在分组中找到对应的位置。（直接用index来定位，避免复杂的hash，B树查找），浪费内存提升效率。

更新一条或者多条元数据时，需要启动事务。事务提交的时候需要同步刷新其他节点的内存信息，如果失败需要进行回滚。




