---
layout: default
title:  "大规模数据中心的分布式文件系统扩展性问题-ATC17论文分析"
date:   2017-09-10
categories: filesystem
---

好久没分析论文了，ATC17论文也出来2个多月，挑几篇相关的阅读下，近期一直在关注分布式的文件系统的思路，正好ATC有一篇：“Scaling Distributed File Systems in Resource-Harvesting Datacenters”，详细分析下看看后续是否有价值参考。

## 1 整体介绍

建议大型数据中心非常昂贵，但是很多时候资源利用率又不高，于是各种应用融合在一起使用数据中心资源成为了一种提高利用率的基本方式。将大量的batch工作同时&分时的跑在一个集群中，充分利用空闲的资源；但是这么做之后，会有一些问题要解决，比如：

- 长期运行的服务需要能够被隔离出来，性能不受影响
- batch任务的执行，也不受长期运行的服务的干扰

前人的工作给了一些方案——History-Based Harvesting of Spare Cycles and Storage in Large-Scale Datacenters，OSDI16，但是这个方案扩展性不够，本文主要解决扩展性问题。

当前的主要分布式文件系统扩展性不过几千节点，一般都需要建很多个资源池来分摊负载，此时需要上层业务来进行业务的切割，管理起来有以下几个问题：

- 用户需要自己了解各个集群的分区和名字空间
- 很容易导致各个集群负载不均衡
- 管理员需要人工的管理数据和名字空间的映射关系
- 当负载很复杂的时候，管理也不知道如何是管理数据

还有一种方式是增强元数据的扩展性，比如GFS的下一代Collossus，不过也有问题：

- 系统会变得非常复杂
- 软件的bugs，失败和管理员误操作会导致非常严重的问题

为了解决以上分布式软件的扩展性问题，论文提出了新的方案：

- 在多个分布式资源池和客户端之间加了一层软件，这个软件可以自动的管理多个子名字空间，最终组成一个巨大的联邦分布式文件系统。
- 核心目标：1）避免多个业务的相互干扰；2）性能优秀，资源利用高，避免负载不平衡

整个方案分2个步骤：

- 将servers分配到不同的子集群，直接使用DHT搞定
- 将目录和文件分配到不同的子集群，并且当某个子集群负载快满的时候，能够进行迁移

实现方法：

- 直接基于HDFS来玩，取了个新名词叫DH-HDFS，选择HDFS主要是因为：1）HDFS用的广泛；2）项目的核心也是想解决分析类负载的问题
- 直接在一个真是的环境中来做实验，在一个巨大的数据中心中，模拟了10个集群；测试结果表现良好，极大的提升了数据的持久度和可用性指标，性能几乎没有损失。
- 当前已经有4个生产环境的使用，最大的集群有超过19k的服务器，6个子集群。
- 当然，这个技术不仅限于HDFS，任何类似的子集群任务切割都会用的上。（比如我们多个CloudFS之间的任务切割也应该用得上）

论文的核心贡献：

- 提出一种扩展分布式文件系统的方法，能够很好的解决海量规模下存储的容量，数据持久度，可用性和性能问题。并且：1）不修改任何现有逻辑，仅仅在客户端和文件系统之间叠加一个中间层；2）一致性hash的方式来创建子集群；3）动态的文件迁移。
- 基于HDFS做了实际的工程实现，并在生产环境使用
- 使用真实负载去测试分析
- 阐述了整个系统的设计经验

## 2 背景

几个定义；

- 首席租户：在大规模数据中心中，大多数服务器分配给原生的，低延迟的负载，由于对于延迟的要求极地，一般直接使用服务器的本地FS，这种负载叫“primary tenant”（首席租户）。
- 二流租户：一些低优先级的业务，比如batch类的分析业务，能否消耗首席租户多余的资源和存储空间
- 首席租户优先级搞，资源不够的时候有限保证他的运行，必要时候可以杀掉二流租户的业务
- 一般来说首席租户拥有这些服务器，并且能否自由的去重新安装业务员和格式化数据

当前很多互联网已经是全共享式数据中心了，比如Google，Facebook，各种不同的业务有不同的优先级。

### 2.1 多样的副本放置方式

数据放置的挑战：

- 如果把所有的首席租户都放在一起，峰值的时候，很容易造成不可用
- 如果管理员或者开发者格式化一个磁盘，这个盘正好包含一个首席租户的所有数据，那么就会造成数据丢失

OSDI16那篇论文给了个解决方案：不允许数据的多个副本落在同一个逻辑的或者物理的集群上（比如物理的rack）

这篇论文也是基于此来做，支持在集群创建了一个集群的联邦，并解决联邦之间设备分配，业务负载均衡，数据迁移等。

### 2.2 大规模的分布式存储

- 大规模：当今各种FS扩展性约束在一个NAME node类似的东西，无法无限的扩展，到了一定程度之后就需要上层业务来切割；有些系统通过mount table的方式做了一些客户不感知的切割，但是mount table无法相互之间协同。
- 负载均衡：在大集群中，负载不均衡是正常的情况，
- HDFS联邦：HDFS有一个自带的联邦功能，但是并没有子集群的概念，需要用户手动的管理多个名字空间，而且所有的心跳还是要同步到所有的manager，扩展性也受限。

## 3 联邦架构

### 3.1 总体架构

![]({{ site.baseurl }}/assets/DH-HDFS-1.png)

- 不修改后台分布式存储系统，仅仅将多个分布式文件系统组成联邦
- 每个子集群相互独立，无需感知对方的存在；每份数据只会存在于一个子集群中
